S3
------------------
store file in buckets(objects)
name must be unique in all the accounts
buckets it's define at the region level
the key is the full path 
s3://mybucket/myfolder/anotherfolder/file.txt
max file 5GB , use multipart upload if bigger and from 100 MB
not pay to upload but pay to download

S3 security
------------------
USER BASED: IAM Policies: allow iam user to access 
RESOURCE BASED: BUCKET Policies: s3 bucket policie, bucket rule to allow user or user from another account (cross account)
                    json based policie
                OACL: object access control list
                BACL: bucket access control list
ENCRYPTION

explicit deny take precedence over an S3 bucket policy 

chaining replication is not okay 


Host static website on S3
403 forbidden make sure the bucket policy allows public reads

Versionning: at the bucket level
    - protect to unintended delete
    - roll back version
version null for file that have no version yet

Replication:
must enable versioning
must give proper access to S3
- CRR cross region replication 
- SRR same region replication 

asynchronous replication
after enable replication only new file are replicated 
S3 Batch replication
only a marker on delete


S3 Storage Classes
------------------

S3 General Purpose: frequently access, big data, mobile & gaming, content distribution
S3 Standard Infrequent Access: less access but require rapid access, lowest cost, disaster recovery 
S3 One Zone Infrequent Access: less access but require rapid access, lowest cost, disaster recovery but in 1 AZ

Archiving Back-up; low-cost price for storage, object retrieval cost
S3 Glacier Instant Revial: millisecond retrieval 
S3 Glacier Flexible Revial: 3 differents tiers
S3 Glacier Deep Archive: 12h to retrieve data
S3 Intelligent Tiering: allow you to move access, 

can move from a class to another, lifecycle configuration 

Standard --> Standard IA --> Intellingent Tiering --> OneZone IA --> Glacier Instant Retrieval --> Glacier Flexible Retrieval --> Glacier Deep Archive

can create lifecycle rules, or expiration rules, or delete version 

S3 analytics -->  csv report with recommandation for standard and standard IA

S3 requester Pays
------------------
those who request the file pay instead of the bucket owner
Owner pay for the storage but the requester pay for the networking cost
The requester must be authenticated to AWS

S3 event notification
------------------
object is created, or removed or ... and react generate thumbnails, and send to sns or sqs or lambda
seconds but sometime minutes

We need to have iam permissions SNS Ressource Access Policy SQS Ressource Access Policy, Lambda Ressource Policy to send event to SQS Lambda and SNS

All event go to eventbridges, and from Eventbridge you can send to 18 aws services (advanced filtering)

S3 Performance
------------------
For each prefix
3500 PUT /s
5500 GET /s

S3 Transfert Acceleration
------------------ 
with help of edge location
compatible with multipart upload
minimize public internet
not pay if the traffic is not accelerated


S3 Byte-Range Fetches
------------------
speed up downloads (GET) in parallels

S3 Select & Glacier Select
------------------
filter before 

S3 Batch Operations
------------------
modify all object at a time, copy object, encrypt all the unecrypted files ...
invoke lambda function ..

S3 Encryption
------------------
- Server side encryption SSE-S3 default 
    - using a key managed by aws, encrypted by aws AES-256,
    - header like "x-amz-server-side-encryption": "AES-256"
- Server side kms/ DSSE-KMS just KMS with double encryption
    - user control of the key
    - audit key with cloudtrail 
    - header like "x-amz-server-side-encryption": "aws:kms"
- Server side customer key
    - https must used, passed the key 
- Client side encryption
    - encrypt before send to Amazon S3

you can force encryption in transit with AWS:secureTransport
bucket policie refuse file without encryption, force away

S3 Cors
------------------
Cross origin resource sharing
Allow origin *

S3 MFA Delete (must enable version), only the root account can do this
force user to generate code to identify (permently delete), suspend versioning

S3 Access logs
log into another bucket
logging loop if the log bucket and the bucket is the same 
search with Athena

S3 presigned URL with permission of the IAM User (temporary access)

S3 Glacier Vault Lock
------------------
Write once, read many and you lock
Vault Lock policy
good for compliance and retention even by admin

S3 object lock (enable first versionning)
for each object not bucket 
- compliance
- governance some person can delete

legal hold: protected forever

S3 Access points
------------------
finance access point, that point for just a folder thanks to a policy
through vpc
vpc endpoint to access the access point
S3 object lambda connected access point to lambda function when retrieved


aws S3 sync: copy object between s3 bucket
S3 console cannot be used to move 1 PB

30 days before moving data into lifecycle oh my god ! 

limit for s3 is per prefix 
and there are no limit of number of prefix
5500 read per prefix, 3500 write per prefix
s3 always return the latest version of an object